# CIFAR-10 Image Classification

This repository contains a deep learning project for classifying images in the CIFAR-10 dataset using a Convolutional Neural Network (CNN) implemented in TensorFlow/Keras.

---

## ğŸ“š Table of Contents

* [Project Overview](#project-overview)
* [Dataset](#dataset)
* [Model Architecture](#model-architecture)
* [Installation](#installation)
* [Usage](#usage)
* [Training Results](#training-results)
* [Key Findings](#key-findings)
* [Next Steps](#next-steps)
* [Folder Structure](#folder-structure)

---

## ğŸ¯ Project Overview

The goal of this project is to build and train a CNN that can accurately classify 32Ã—32 color images into one of 10 categories (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).

The workflow includes:

1. Loading and preprocessing the CIFAR-10 dataset.
2. Defining a CNN architecture.
3. Training the model with real-time data augmentation.
4. Evaluating performance on validation data.
5. Visualizing loss and accuracy curves.

---

## ğŸ—‚ï¸ Folder Structure

```plaintext
â”œâ”€â”€ figures/                 # Generated plots (loss_accuracy.png)
â”œâ”€â”€ Hakim_Murphy_CV_cifar10.ipynb  # Jupyter notebook
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # This file
â””â”€â”€ .gitignore               # Git ignore rules
```

---

## ğŸ—ƒï¸ Dataset

* **CIFAR-10**: 60,000 images (50K train / 10K test)
* 10 classes, 6,000 images per class
* Images are 32Ã—32 pixels with three color channels (RGB)

Built-in Keras utility is used to download and preprocess the data.

--- 

## ğŸ—ï¸ Model Architecture

The CNN consists of:

* **Conv Blocks**: Multiple convolutional layers with ReLU activation, followed by max pooling.
* **Batch Normalization**: To accelerate convergence and improve stability.
* **Dropout Layers**: For regularization.
* **Fully Connected Layers**: To map to the 10 output classes.

A summary of key layers:

```plaintext
Input: 32Ã—32Ã—3 image
Pretrained ResNet50 (include
_
top=False) backbone
Flatten output maps
Dense(512, relu) â†’ BatchNorm â†’ Dropout(0.5)
Dense(256, relu) â†’ BatchNorm â†’ Dropout(0.2)
Dense(10, softmax) output
```

--- 

## ğŸ› ï¸ Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/cifar10-classification.git
   cd cifar10-classification
   ```

2. Create a virtual environment and activate it:

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\\Scripts\\activate   # Windows
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

---

## â–¶ï¸ Usage

1. Open the Jupyter notebook:

   ```bash
   jupyter notebook Hakim_Murphy_CV_cifar10.ipynb
   ```

2. Run all cells to:

   * Load and preprocess data
   * Build and compile the model
   * Train with data augmentation
   * Plot loss and accuracy curves

3. Adjust hyperparameters (learning rate, batch size, epochs) in the notebook as needed.

---

## ğŸ“ˆ Training Results (5 runs)

After training for 10 epochs, the models achieved:

* **Test Loss**: 1.72 Â± 0.81  
* **Test Accuracy**: 42.4 Â± 26.0 %

---

## ğŸ” Key Findings

* **Fast initial learning**: Rapid drop in loss and rise in accuracy in the first 5â€“6 epochs.
* **Healthy generalization** through epoch 6: Training and validation metrics closely track.
* **Mild overfitting** after epoch 6: Validation plateaus while training improves.

---

## ğŸ§­ Next Steps

* Implement **EarlyStopping** callback to halt training when validation stops improving.
* Apply stronger **regularization** (dropout, weight decay) or **augmentations** to reduce overfitting.
* Introduce a **learning rate scheduler** (e.g., ReduceLROnPlateau).
* Save the best model weights using **ModelCheckpoint**.

---

## ğŸ“„ License
MIT

---

## ğŸ—£ï¸ Author
Hakim Murphy


